apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2025-07-21T06:52:28Z"
    generateName: alertmanager-monitoring-kube-prometheus-alertmanager-
    labels:
      alertmanager: monitoring-kube-prometheus-alertmanager
      app.kubernetes.io/instance: monitoring-kube-prometheus-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.28.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-monitoring-kube-prometheus-alertmanager-57594fd46
      statefulset.kubernetes.io/pod-name: alertmanager-monitoring-kube-prometheus-alertmanager-0
    name: alertmanager-monitoring-kube-prometheus-alertmanager-0
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-monitoring-kube-prometheus-alertmanager
      uid: ead7667f-f75b-4b00-aacb-a9a0dbe54cc0
    resourceVersion: "1720479"
    uid: 057fccb5-0ed9-48f5-b999-7eeff5dc2495
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - monitoring-kube-prometheus-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://monitoring-kube-prometheus-alertmanager.default:9093
      - --web.route-prefix=/
      - --cluster.label=default/monitoring-kube-prometheus-alertmanager
      - --cluster.peer=alertmanager-monitoring-kube-prometheus-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.28.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-monitoring-kube-prometheus-alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        subPath: cluster-tls-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4w6r4
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4w6r4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-monitoring-kube-prometheus-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4w6r4
        readOnly: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: monitoring-kube-prometheus-alertmanager
    serviceAccountName: monitoring-kube-prometheus-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-monitoring-kube-prometheus-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-monitoring-kube-prometheus-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-monitoring-kube-prometheus-alertmanager-web-config
    - name: cluster-tls-config
      secret:
        defaultMode: 420
        secretName: alertmanager-monitoring-kube-prometheus-alertmanager-cluster-tls-config
    - emptyDir: {}
      name: alertmanager-monitoring-kube-prometheus-alertmanager-db
    - name: kube-api-access-4w6r4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a699f6e6775cf5f7fe167297e37c4df3a3a726c75cd8704a1058867932724bde
      image: quay.io/prometheus/alertmanager:v0.28.1
      imageID: quay.io/prometheus/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba
      lastState:
        terminated:
          containerID: containerd://b591e798dc5a231f3bd3c87655847e360117eda5e4abd4c0b5fdde55aa0e5266
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          message: |
            time=2025-08-11T10:17:13.066Z level=INFO source=main.go:191 msg="Starting Alertmanager" version="(version=0.28.1, branch=HEAD, revision=b2099eaa2c9ebc25edb26517cb9c732738e93910)"
            time=2025-08-11T10:17:13.067Z level=INFO source=main.go:192 msg="Build context" build_context="(go=go1.23.7, platform=linux/amd64, user=root@fa3ca569dfe4, date=20250307-15:05:18, tags=netgo)"
            time=2025-08-11T10:17:13.552Z level=INFO source=coordinator.go:112 msg="Loading configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
            time=2025-08-11T10:17:13.553Z level=INFO source=coordinator.go:125 msg="Completed loading of configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
            time=2025-08-11T10:17:13.694Z level=INFO source=tls_config.go:347 msg="Listening on" address=[::]:9093
            time=2025-08-11T10:17:13.694Z level=INFO source=tls_config.go:386 msg="TLS is disabled." http2=false address=[::]:9093
            time=2025-08-11T10:17:19.534Z level=INFO source=coordinator.go:112 msg="Loading configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
            time=2025-08-11T10:17:19.535Z level=INFO source=coordinator.go:125 msg="Completed loading of configuration file" component=configuration file=/etc/alertmanager/config_out/alertmanager.env.yaml
          reason: Unknown
          startedAt: "2025-08-11T10:16:48Z"
      name: alertmanager
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:46Z"
    - containerID: containerd://2eead2bee4b3d8a2f4d27bd7bb9ed29dce7ddd1d29e07fdaa79be8cfc76dcf1b
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef
      lastState:
        terminated:
          containerID: containerd://6353497cb074ea1e6627b1acca9a8fbd1eddd7482539beae0e681084f55361c5
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          message: |
            ts=2025-08-11T10:17:19.402054572Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:148 msg="Starting prometheus-config-reloader" version="(version=0.83.0, branch=, revision=5cf2f5d)" build_context="(go=go1.24.3, platform=linux/amd64, user=, date=20250530-07:46:40, tags=unknown)"
            ts=2025-08-11T10:17:19.40310979Z level=info caller=/workspace/internal/goruntime/cpu.go:27 msg="Leaving GOMAXPROCS=2: CPU quota undefined"
            level=info ts=2025-08-11T10:17:19.40408689Z caller=reloader.go:282 msg="reloading via HTTP"
            ts=2025-08-11T10:17:19.404267547Z level=info caller=/workspace/cmd/prometheus-config-reloader/main.go:202 msg="Starting web server for metrics" listen=:8080
            ts=2025-08-11T10:17:19.405676577Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:347 msg="Listening on" address=[::]:8080
            ts=2025-08-11T10:17:19.405852123Z level=info caller=/go/pkg/mod/github.com/prometheus/exporter-toolkit@v0.14.0/web/tls_config.go:386 msg="TLS is disabled." http2=false address=[::]:8080
            level=info ts=2025-08-11T10:17:19.537355363Z caller=reloader.go:548 msg="Reload triggered" cfg_in=/etc/alertmanager/config/alertmanager.yaml.gz cfg_out=/etc/alertmanager/config_out/alertmanager.env.yaml cfg_dirs= watched_dirs=/etc/alertmanager/config
            level=info ts=2025-08-11T10:17:19.537500386Z caller=reloader.go:330 msg="started watching config file and directories for changes" cfg=/etc/alertmanager/config/alertmanager.yaml.gz cfgDirs= out=/etc/alertmanager/config_out/alertmanager.env.yaml dirs=/etc/alertmanager/config
          reason: Unknown
          startedAt: "2025-08-11T10:17:19Z"
      name: config-reloader
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:50Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    initContainerStatuses:
    - containerID: containerd://b780362dc90f8f058bb057a46938fd3f29aa5c7b8b03472c56789dc913e309fa
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 10
      started: false
      state:
        terminated:
          containerID: containerd://b780362dc90f8f058bb057a46938fd3f29aa5c7b8b03472c56789dc913e309fa
          exitCode: 0
          finishedAt: "2025-09-29T11:42:40Z"
          reason: Completed
          startedAt: "2025-09-29T11:42:06Z"
    phase: Running
    podIP: 10.244.0.13
    podIPs:
    - ip: 10.244.0.13
    qosClass: Burstable
    startTime: "2025-07-21T06:52:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
      checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
      checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
      kubectl.kubernetes.io/default-container: grafana
    creationTimestamp: "2025-08-01T09:44:18Z"
    generateName: monitoring-grafana-7cd95b6c46-
    labels:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.2
      helm.sh/chart: grafana-9.2.10
      pod-template-hash: 7cd95b6c46
    name: monitoring-grafana-7cd95b6c46-2qb2d
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: monitoring-grafana-7cd95b6c46
      uid: e75cdbf5-d954-4e48-9750-040e8de25433
    resourceVersion: "1720618"
    uid: 29c3cfb7-1093-43d1-9639-6dc220f368e1
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /tmp/dashboards
      - name: RESOURCE
        value: both
      - name: NAMESPACE
        value: ALL
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: monitoring-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: monitoring-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/dashboards/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.30.3
      imagePullPolicy: IfNotPresent
      name: grafana-sc-dashboard
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptb2c
        readOnly: true
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: LABEL_VALUE
        value: "1"
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: monitoring-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: monitoring-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.30.3
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptb2c
        readOnly: true
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: monitoring-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: monitoring-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: docker.io/grafana/grafana:12.0.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      - containerPort: 9094
        name: gossip-tcp
        protocol: TCP
      - containerPort: 9094
        name: gossip-udp
        protocol: UDP
      - containerPort: 6060
        name: profiling
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /tmp/dashboards
        name: sc-dashboard-volume
      - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
        name: sc-dashboard-provider
        subPath: provider.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ptb2c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsNonRoot: true
      runAsUser: 472
    serviceAccount: monitoring-grafana
    serviceAccountName: monitoring-grafana
    shareProcessNamespace: false
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: monitoring-grafana
      name: config
    - emptyDir: {}
      name: storage
    - emptyDir: {}
      name: sc-dashboard-volume
    - configMap:
        defaultMode: 420
        name: monitoring-grafana-config-dashboards
      name: sc-dashboard-provider
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-ptb2c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:42Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-08-01T09:44:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:44:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:44:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-08-01T09:44:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://deb8a5cf0202cd5e2aff707477011ab75a6f17d18886536b8e790a21a07cea7f
      image: docker.io/grafana/grafana:12.0.2
      imageID: docker.io/grafana/grafana@sha256:b5b59bfc7561634c2d7b136c4543d702ebcc94a3da477f21ff26f89ffd4214fa
      lastState:
        terminated:
          containerID: containerd://9e042d62277896e594895dbaae8f9343af87102d04f5816b08c5e990a6effa0e
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:16:52Z"
      name: grafana
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:41Z"
    - containerID: containerd://24aa01e10f34f8e4e200cb5a04d4b38892385c6ed998976487dab0ed41e1265e
      image: quay.io/kiwigrid/k8s-sidecar:1.30.3
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:49dcce269568b1645b0050f296da787c99119647965229919a136614123f0627
      lastState:
        terminated:
          containerID: containerd://14a244aae5f4d60fa33a97d6537cfa5b30115a8c1b7be3fe10b5c4034b449026
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:16:04Z"
      name: grafana-sc-dashboard
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:07Z"
    - containerID: containerd://c1f650a7d0f13de171ce96328b9bab2af3828e69874d94c19fe315ff70f11944
      image: quay.io/kiwigrid/k8s-sidecar:1.30.3
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:49dcce269568b1645b0050f296da787c99119647965229919a136614123f0627
      lastState:
        terminated:
          containerID: containerd://48da8006d0d63baade552cf86523b8f09abae6b276310518adf43e9c1bd1482c
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:16:34Z"
      name: grafana-sc-datasources
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:29Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.5
    podIPs:
    - ip: 10.244.0.5
    qosClass: BestEffort
    startTime: "2025-08-01T09:44:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-21T06:52:10Z"
    generateName: monitoring-kube-prometheus-operator-74f7694f79-
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      pod-template-hash: 74f7694f79
      release: monitoring
    name: monitoring-kube-prometheus-operator-74f7694f79-pnrs5
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: monitoring-kube-prometheus-operator-74f7694f79
      uid: e99e07bd-9f49-4ef4-8ffe-b6a69b1e8ac8
    resourceVersion: "1720500"
    uid: 6af59ad8-d0cc-4149-9db1-cc996e1b7b93
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/monitoring-kube-prometheus-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.1
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.83.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l6szf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: monitoring-kube-prometheus-operator
    serviceAccountName: monitoring-kube-prometheus-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: monitoring-kube-prometheus-admission
    - name: kube-api-access-l6szf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a347c89101605fb6d17aef8ca978bb95cf56beea1222a8f24cbb4058599af3ac
      image: quay.io/prometheus-operator/prometheus-operator:v0.83.0
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:b6a89b8ec08f4cca759b2d579e8545f97ffb897973fcd68148c153f2e936c8b3
      lastState:
        terminated:
          containerID: containerd://b48589eb177caf37e423aa9b01e2956c628c81b11ad13ed001996af60c3a29cb
          exitCode: 2
          finishedAt: "2025-09-29T11:43:00Z"
          reason: Error
          startedAt: "2025-09-29T11:42:38Z"
      name: kube-prometheus-stack
      ready: true
      restartCount: 35
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:43:02Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.3
    podIPs:
    - ip: 10.244.0.3
    qosClass: BestEffort
    startTime: "2025-07-21T06:52:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-21T06:52:10Z"
    generateName: monitoring-kube-state-metrics-877c9547f-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.16.0
      helm.sh/chart: kube-state-metrics-6.1.0
      pod-template-hash: 877c9547f
      release: monitoring
    name: monitoring-kube-state-metrics-877c9547f-4dpmp
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: monitoring-kube-state-metrics-877c9547f
      uid: c8cbf224-bc1c-4590-9d93-8616a3abb5f2
    resourceVersion: "1720456"
    uid: 46e7a0b6-59a2-4890-96e7-436439bbd0c7
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4t59d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: monitoring-kube-state-metrics
    serviceAccountName: monitoring-kube-state-metrics
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-4t59d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://630af902efc90295c056153374ed250653984fec46e150ad845d634f1a18b8e0
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:e750cd4b43f782e3106537026c2995cac85d921aedea334e1d16caad7877c360
      lastState:
        terminated:
          containerID: containerd://a41e277e2d9a318380acf89bd3a89414c16cf73cb2c0cbce86b850b2a85bd6ab
          exitCode: 2
          finishedAt: "2025-09-29T11:42:40Z"
          reason: Error
          startedAt: "2025-09-29T11:42:07Z"
      name: kube-state-metrics
      ready: true
      restartCount: 603
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:46Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.11
    podIPs:
    - ip: 10.244.0.11
    qosClass: BestEffort
    startTime: "2025-07-21T06:52:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-07-21T06:52:10Z"
    generateName: monitoring-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.1
      controller-revision-hash: 76468574dc
      helm.sh/chart: prometheus-node-exporter-4.47.1
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: monitoring
    name: monitoring-prometheus-node-exporter-gmwjs
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: monitoring-prometheus-node-exporter
      uid: 74d257ad-f141-404b-b641-5a4b0026bba5
    resourceVersion: "1720288"
    uid: 04abbe0f-5518-4cb2-8855-facb6503375b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - my-lab-control-plane
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.9.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: my-lab-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: monitoring-prometheus-node-exporter
    serviceAccountName: monitoring-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a0ab5905fc70a98ad38ae6fc89f91ff4951941e5a4a1654ea0f355afcbf19614
      image: quay.io/prometheus/node-exporter:v1.9.1
      imageID: quay.io/prometheus/node-exporter@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a
      lastState:
        terminated:
          containerID: containerd://682527770dd5b0f0d77ce8bc73a85917085b67aac0fe0dff8e4ef66e623b3e25
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:34Z"
      name: node-exporter
      ready: true
      restartCount: 11
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:05Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: BestEffort
    startTime: "2025-07-21T06:52:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"oom-demo","namespace":"default"},"spec":{"containers":[{"command":["sh","-c","x=0; while true; do x=$x$x; done"],"image":"busybox","name":"memory-hog","resources":{"limits":{"cpu":"100m","memory":"20Mi"}}}]}}
    creationTimestamp: "2025-08-07T12:45:21Z"
    name: oom-demo
    namespace: default
    resourceVersion: "1931911"
    uid: f12d99ea-663e-4f4c-aa4b-1627462b65dc
  spec:
    containers:
    - command:
      - sh
      - -c
      - x=0; while true; do x=$x$x; done
      image: busybox
      imagePullPolicy: Always
      name: memory-hog
      resources:
        limits:
          cpu: 100m
          memory: 20Mi
        requests:
          cpu: 100m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cvs2d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-cvs2d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:44Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-08-07T12:45:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-10-01T08:29:10Z"
      message: 'containers with unready status: [memory-hog]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-10-01T08:29:10Z"
      message: 'containers with unready status: [memory-hog]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-08-07T12:45:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a77a3997002493d84be86e813f4cbe377e2cbefa0d858f0a0fc4eab6ea193947
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:d82f458899c9696cb26a7c02d5568f81c8c8223f8661bb2a7988b269c8b9051e
      lastState:
        terminated:
          containerID: containerd://a77a3997002493d84be86e813f4cbe377e2cbefa0d858f0a0fc4eab6ea193947
          exitCode: 137
          finishedAt: "2025-10-01T08:29:10Z"
          reason: OOMKilled
          startedAt: "2025-10-01T08:29:08Z"
      name: memory-hog
      ready: false
      restartCount: 2126
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=memory-hog pod=oom-demo_default(f12d99ea-663e-4f4c-aa4b-1627462b65dc)
          reason: CrashLoopBackOff
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.6
    podIPs:
    - ip: 10.244.0.6
    qosClass: Guaranteed
    startTime: "2025-08-07T12:45:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2025-07-21T06:52:29Z"
    generateName: prometheus-monitoring-kube-prometheus-prometheus-
    labels:
      app.kubernetes.io/instance: monitoring-kube-prometheus-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 3.5.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-monitoring-kube-prometheus-prometheus-95f6cf48c
      operator.prometheus.io/name: monitoring-kube-prometheus-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: monitoring-kube-prometheus-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-monitoring-kube-prometheus-prometheus-0
    name: prometheus-monitoring-kube-prometheus-prometheus-0
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-monitoring-kube-prometheus-prometheus
      uid: 4b873798-1c31-495b-b26d-68f602a59c55
    resourceVersion: "1720588"
    uid: db92d487-35c4-4b6a-a4c8-431c80c4506a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - monitoring-kube-prometheus-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://monitoring-kube-prometheus-prometheus.default:9090
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=10d
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v3.5.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-monitoring-kube-prometheus-prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vtcnw
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vtcnw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-monitoring-kube-prometheus-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
        name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vtcnw
        readOnly: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: monitoring-kube-prometheus-prometheus
    serviceAccountName: monitoring-kube-prometheus-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-monitoring-kube-prometheus-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-monitoring-kube-prometheus-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
      name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-monitoring-kube-prometheus-prometheus-web-config
    - emptyDir: {}
      name: prometheus-monitoring-kube-prometheus-prometheus-db
    - name: kube-api-access-vtcnw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T06:52:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ca11298e77948211fafc716dcac52a3f9a023ea0bdfb817ae6c7e39d9316453c
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef
      lastState:
        terminated:
          containerID: containerd://fa62e22397b17a070057d7019b911488aa68cd2ae6ee90d6083c8fbcc10d1c74
          exitCode: 255
          finishedAt: "2025-09-29T11:41:18Z"
          message: |
            " err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=error ts=2025-08-11T10:17:42.050912075Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=error ts=2025-08-11T10:17:47.050972867Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=error ts=2025-08-11T10:17:52.050709538Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=error ts=2025-08-11T10:17:57.05222139Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=error ts=2025-08-11T10:18:32.081589846Z caller=runutil.go:117 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
            level=info ts=2025-08-11T10:18:42.794904508Z caller=reloader.go:548 msg="Reload triggered" cfg_in=/etc/prometheus/config/prometheus.yaml.gz cfg_out=/etc/prometheus/config_out/prometheus.env.yaml cfg_dirs= watched_dirs=/etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
            level=info ts=2025-08-11T10:18:42.795159638Z caller=reloader.go:330 msg="started watching config file and directories for changes" cfg=/etc/prometheus/config/prometheus.yaml.gz cfgDirs= out=/etc/prometheus/config_out/prometheus.env.yaml dirs=/etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
          reason: Unknown
          startedAt: "2025-08-11T10:17:11Z"
      name: config-reloader
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:52Z"
    - containerID: containerd://d73e961fdc4554e8f6ffde8a0a1c68326e6782f6d024331ba278c69abd32e2b8
      image: quay.io/prometheus/prometheus:v3.5.0
      imageID: quay.io/prometheus/prometheus@sha256:63805ebb8d2b3920190daf1cb14a60871b16fd38bed42b857a3182bc621f4996
      lastState:
        terminated:
          containerID: containerd://cfecdfb2d25076d16ca03e52735edb193b18c3f48a8d1a0de3ecb0cbfc231cde
          exitCode: 255
          finishedAt: "2025-09-29T11:41:18Z"
          message: |
            =1755079200000 ulid=01K2HHCFMQKDH6CR1ZNCAXXNK6 duration=603.79235ms ooo=false
            time=2025-08-13T11:00:01.435Z level=INFO source=head.go:1410 msg="Head GC completed" component=tsdb caller=truncateMemory duration=36.447295ms
            time=2025-08-13T11:00:03.474Z level=INFO source=compact.go:545 msg="compact blocks" component=tsdb count=3 mint=1755043200341 maxt=1755064800000 ulid=01K2HHCG90WMSC7NTQJZSPGEF5 sources="[01K2GNXJMQXFVQ586B9FKQZHBC 01K2GWS9F0BQS6NEYAVPXDQ75C 01K2H3N0Q1HC29D652VQMY39TR]" duration=2.034048663s
            time=2025-08-13T11:00:03.486Z level=INFO source=db.go:1814 msg="Deleting obsolete block" component=tsdb block=01K2H3N0Q1HC29D652VQMY39TR
            time=2025-08-13T11:00:03.493Z level=INFO source=db.go:1814 msg="Deleting obsolete block" component=tsdb block=01K2GWS9F0BQS6NEYAVPXDQ75C
            time=2025-08-13T11:00:03.500Z level=INFO source=db.go:1814 msg="Deleting obsolete block" component=tsdb block=01K2GNXJMQXFVQ586B9FKQZHBC
            time=2025-08-13T13:00:01.121Z level=INFO source=compact.go:606 msg="write block" component=tsdb mint=1755079200173 maxt=1755086400000 ulid=01K2HR86F3C4WJ2KA4X06YWF6B duration=765.866021ms ooo=false
            time=2025-08-13T13:00:01.202Z level=INFO source=db.go:1814 msg="Deleting obsolete block" component=tsdb block=01K1TBT3MJJPJXDZG7Z6Q63GNW
            time=2025-08-13T13:00:01.248Z level=INFO source=head.go:1410 msg="Head GC completed" component=tsdb caller=truncateMemory duration=45.607145ms
            time=2025-08-13T13:00:01.253Z level=INFO source=checkpoint.go:100 msg="Creating checkpoint" component=tsdb from_segment=199 to_segment=200 mint=1755086400000
            time=2025-08-13T13:00:02.771Z level=INFO source=head.go:1372 msg="WAL checkpoint complete" component=tsdb first=199 last=200 duration=1.518970686s
            time=2025-08-13T15:00:01.218Z level=INFO source=compact.go:606 msg="write block" component=tsdb mint=1755086400259 maxt=1755093600000 ulid=01K2HZ3XQ0R9CP7Y1AYTXS27CK duration=866.456711ms ooo=false
            time=2025-08-13T15:00:01.253Z level=INFO source=head.go:1410 msg="Head GC completed" component=tsdb caller=truncateMemory duration=31.838596ms
          reason: Unknown
          startedAt: "2025-08-11T10:16:47Z"
      name: prometheus
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:48Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    initContainerStatuses:
    - containerID: containerd://f7bb099b046625c9469c77338cd37f36d9dc8bdeead859701f89658337574852
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:78aec597d87aa2b4ba947ab9190538dae93a58a67b8e930aefea1086534b02ef
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 10
      started: false
      state:
        terminated:
          containerID: containerd://f7bb099b046625c9469c77338cd37f36d9dc8bdeead859701f89658337574852
          exitCode: 0
          finishedAt: "2025-09-29T11:42:40Z"
          reason: Completed
          startedAt: "2025-09-29T11:42:06Z"
    phase: Running
    podIP: 10.244.0.14
    podIPs:
    - ip: 10.244.0.14
    qosClass: BestEffort
    startTime: "2025-07-21T06:52:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-08-11T10:42:04Z"
    generateName: coredns-7db6d8ff4d-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 7db6d8ff4d
    name: coredns-7db6d8ff4d-l6j6r
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-7db6d8ff4d
      uid: f2ee9818-0858-4ad1-bca5-98e85e80b9a2
    resourceVersion: "1720531"
    uid: d2e7cfba-100f-4441-b5df-719085651894
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h4pdj
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-h4pdj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-08-11T10:42:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-08-11T10:42:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c5e9d7e85b21796880816d1bf33582b9b1805554989a2d1605c68d49cedfdba5
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: sha256:cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4
      lastState:
        terminated:
          containerID: containerd://95e7442356a40e3ca64c8457e5d2501abe81e5a3280bae4cae9bdeb60cbf60d0
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:42:16Z"
      name: coredns
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:05Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.9
    podIPs:
    - ip: 10.244.0.9
    qosClass: Burstable
    startTime: "2025-08-11T10:42:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-08-11T10:42:05Z"
    generateName: coredns-7db6d8ff4d-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 7db6d8ff4d
    name: coredns-7db6d8ff4d-rcl9x
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-7db6d8ff4d
      uid: f2ee9818-0858-4ad1-bca5-98e85e80b9a2
    resourceVersion: "1720515"
    uid: b1fc4803-a025-4aec-a394-658205922d08
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l5vqh
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-l5vqh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-08-11T10:42:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-08-11T10:42:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8cae8174078426e6fb5ef968da36c93141c4fce9a057f116ad82a01a0e2b4ae4
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: sha256:cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4
      lastState:
        terminated:
          containerID: containerd://5a2ac42c5353dcd054716520e3a259fa869de74d676af55d7ab95622a632bed5
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:42:17Z"
      name: coredns
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:06Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.8
    podIPs:
    - ip: 10.244.0.8
    qosClass: Burstable
    startTime: "2025-08-11T10:42:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.2:2379
      kubernetes.io/config.hash: 8759e1428b09c893eea5e36bbec37cf5
      kubernetes.io/config.mirror: 8759e1428b09c893eea5e36bbec37cf5
      kubernetes.io/config.seen: "2025-07-21T03:46:41.403486293Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-07-21T03:46:41Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-my-lab-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: my-lab-control-plane
      uid: a9cb0c66-f155-4e96-b603-97ffb0dcb70e
    resourceVersion: "1720153"
    uid: 5801effb-5345-4fb4-a66f-728e15735fcf
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://172.18.0.2:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --experimental-initial-corrupt-check=true
      - --experimental-watch-progress-notify-interval=5s
      - --initial-advertise-peer-urls=https://172.18.0.2:2380
      - --initial-cluster=my-lab-control-plane=https://172.18.0.2:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.2:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://172.18.0.2:2380
      - --name=my-lab-control-plane
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: registry.k8s.io/etcd:3.5.12-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health?exclude=NOSPACE&serializable=true
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /health?serializable=false
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ebbdf27133643409e27b54bdc222ac0381bcd6f4e1de466eb1ab652d78ff487a
      image: registry.k8s.io/etcd:3.5.12-0
      imageID: sha256:3861cfcd7c04ccac1f062788eca39487248527ef0c0cfd477a83d7691a75a899
      lastState:
        terminated:
          containerID: containerd://2fac0083f992f2eae404891c5804d16d0f2f1163d4375310ed346ef4117248c9
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:12Z"
      name: etcd
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:41:30Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Burstable
    startTime: "2025-09-29T11:41:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-21T03:46:55Z"
    generateName: kindnet-
    labels:
      app: kindnet
      controller-revision-hash: 859b87c647
      k8s-app: kindnet
      pod-template-generation: "1"
      tier: node
    name: kindnet-xjsxx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kindnet
      uid: e3287825-7588-414a-b428-fc8fb2f95c71
    resourceVersion: "1720230"
    uid: be22bb3e-d7c0-41e8-b35c-d248167bac6b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - my-lab-control-plane
    containers:
    - env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_SUBNET
        value: 10.244.0.0/16
      - name: CONTROL_PLANE_ENDPOINT
        value: my-lab-control-plane:6443
      image: docker.io/kindest/kindnetd:v20240202-8f1494ea
      imagePullPolicy: IfNotPresent
      name: kindnet-cni
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_RAW
          - NET_ADMIN
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s8zlx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: my-lab-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kindnet
    serviceAccountName: kindnet
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-s8zlx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T03:46:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T03:46:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3605ebce2496757780253409d5e8067dcaae59a3d38ec7f05391647f629b07e5
      image: docker.io/kindest/kindnetd:v20240202-8f1494ea
      imageID: sha256:4950bb10b3f87e8d4a8f772a0d8934625cac4ccfa3675fea34cad0dab83fd5a5
      lastState:
        terminated:
          containerID: containerd://0c334b94f067df010cbaf8077093aac979c187a0d1aa90636cbcb0377ca7ac23
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:34Z"
      name: kindnet-cni
      ready: true
      restartCount: 137
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:05Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Guaranteed
    startTime: "2025-07-21T03:46:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.2:6443
      kubernetes.io/config.hash: 2aa2fff09d88ec630d14d4355a1f6bcd
      kubernetes.io/config.mirror: 2aa2fff09d88ec630d14d4355a1f6bcd
      kubernetes.io/config.seen: "2025-07-21T03:46:41.403488430Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-07-21T03:46:41Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-my-lab-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: my-lab-control-plane
      uid: a9cb0c66-f155-4e96-b603-97ffb0dcb70e
    resourceVersion: "1720187"
    uid: 1f6dc146-52d2-48d5-817f-54ccabc5c2ea
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=172.18.0.2
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --runtime-config=
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/16
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      image: registry.k8s.io/kube-apiserver:v1.30.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 172.18.0.2
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 172.18.0.2
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 172.18.0.2
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://138e8ce4b6b198d96cc86706e0e69194192054881bd2620ecd4581daedc77c1a
      image: registry.k8s.io/kube-apiserver-amd64:v1.30.0
      imageID: docker.io/library/import-2024-05-13@sha256:d147453847b7c5b8c33045e54d6e0301edea08b5ae152e84b9b5cab377e20321
      lastState:
        terminated:
          containerID: containerd://3188699699640a47bf1a8774f716b4a46a11e0e3134a49bf7e33bff9eeb3c7a3
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:12Z"
      name: kube-apiserver
      ready: true
      restartCount: 106
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:41:30Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Burstable
    startTime: "2025-09-29T11:41:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 646702c16e5b1c27768bfaec17eee612
      kubernetes.io/config.mirror: 646702c16e5b1c27768bfaec17eee612
      kubernetes.io/config.seen: "2025-07-21T03:46:41.403476438Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-07-21T03:46:41Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-my-lab-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: my-lab-control-plane
      uid: a9cb0c66-f155-4e96-b603-97ffb0dcb70e
    resourceVersion: "1720150"
    uid: 9aa97969-9877-47c5-9a14-116552b3cf30
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cluster-cidr=10.244.0.0/16
      - --cluster-name=my-lab
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=*,bootstrapsigner,tokencleaner
      - --enable-hostpath-provisioner=true
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/16
      - --use-service-account-credentials=true
      image: registry.k8s.io/kube-controller-manager:v1.30.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9b6527cd1be48de91e2523c99e1edc8dae5bd775d372a65e84f4ab0bd06a0239
      image: registry.k8s.io/kube-controller-manager-amd64:v1.30.0
      imageID: docker.io/library/import-2024-05-13@sha256:9615dec64d132ce44d42c15d071da7477499ab2aa9a44ff073b8d85944949023
      lastState:
        terminated:
          containerID: containerd://734ee4b9a5e30c62c4c5a15eb2244f181c27e8f625a4d5515f2309f6668bf817
          exitCode: 255
          finishedAt: "2025-09-29T11:41:18Z"
          reason: Unknown
          startedAt: "2025-08-12T03:42:10Z"
      name: kube-controller-manager
      ready: true
      restartCount: 383
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:41:30Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Burstable
    startTime: "2025-09-29T11:41:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-21T03:46:55Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 79cf874c65
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-wfc6p
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 1d482b88-44ce-46e5-a54d-a7be30f3d72c
    resourceVersion: "1720232"
    uid: 9272962c-fdb1-49da-9c9d-5ad31837dcb1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - my-lab-control-plane
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.30.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nj4dd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: my-lab-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-nj4dd
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T03:46:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T03:46:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://fbb251bdc666aad0769e0e7bc91cc95119f92ef9bd30ecad1ff51bc7edaf99df
      image: registry.k8s.io/kube-proxy-amd64:v1.30.0
      imageID: docker.io/library/import-2024-05-13@sha256:98e54aaf0614547ead5c0bd7df946f6eb58f0abf0386240df20565d9dd2a2e8d
      lastState:
        terminated:
          containerID: containerd://8e874ac531bbf75fb052044048ee73a1b56da6cdf086259c2fb805118c96aff5
          exitCode: 255
          finishedAt: "2025-09-29T11:41:18Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:34Z"
      name: kube-proxy
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:05Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: BestEffort
    startTime: "2025-07-21T03:46:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 4af1c119b0e25c0042670ee354bd80eb
      kubernetes.io/config.mirror: 4af1c119b0e25c0042670ee354bd80eb
      kubernetes.io/config.seen: "2025-07-21T03:46:41.403484017Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-07-21T03:46:41Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-my-lab-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: my-lab-control-plane
      uid: a9cb0c66-f155-4e96-b603-97ffb0dcb70e
    resourceVersion: "1720152"
    uid: 13a3bde5-d93c-4167-ab41-4006736a6def
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      image: registry.k8s.io/kube-scheduler:v1.30.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:41:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f6fb7fd26d818e52f9fc54c550cade4811c18684028049ea573c9a8d85f97f46
      image: registry.k8s.io/kube-scheduler-amd64:v1.30.0
      imageID: docker.io/library/import-2024-05-13@sha256:06e68f241fc2e56756800474858d7b359f7cf4da31eddae316a342059c20c274
      lastState:
        terminated:
          containerID: containerd://812f026254f1b90d43ab78006aaab7a21c8f2140699a04a44f9afd3ea0d5fce2
          exitCode: 255
          finishedAt: "2025-09-29T11:41:18Z"
          reason: Unknown
          startedAt: "2025-08-12T03:42:14Z"
      name: kube-scheduler
      ready: true
      restartCount: 384
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:41:30Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Burstable
    startTime: "2025-09-29T11:41:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-21T03:46:55Z"
    generateName: local-path-provisioner-988d74bc-
    labels:
      app: local-path-provisioner
      pod-template-hash: 988d74bc
    name: local-path-provisioner-988d74bc-j2bps
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-988d74bc
      uid: 1b33bd94-1612-4b8d-b627-c72daf84433c
    resourceVersion: "1720540"
    uid: b1e17a3e-9461-4140-b20f-b8811dad0384
  spec:
    containers:
    - command:
      - local-path-provisioner
      - --debug
      - start
      - --helper-image
      - docker.io/kindest/local-path-helper:v20230510-486859a6
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: docker.io/kindest/local-path-provisioner:v20240202-8f1494ea
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-746tr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Equal
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Equal
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-746tr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T03:46:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:43:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-21T03:46:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f0f71e7e3f6a710331e7911384449c3c20aac3bacab7a3a265a248a3ee1bdf94
      image: docker.io/kindest/local-path-provisioner:v20240202-8f1494ea
      imageID: sha256:0500518ebaa68d16973c65dc0b776813b50ab6e7e8f112fca41aca387a549d4f
      lastState:
        terminated:
          containerID: containerd://b37ce50dabdcf117f5094872d0562b7e388c0eb1efd22849e55e09184f97f32e
          exitCode: 1
          finishedAt: "2025-09-29T11:43:02Z"
          reason: Error
          startedAt: "2025-09-29T11:42:07Z"
      name: local-path-provisioner
      ready: true
      restartCount: 20
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:43:19Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.7
    podIPs:
    - ip: 10.244.0.7
    qosClass: BestEffort
    startTime: "2025-07-21T03:46:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-22T06:52:10Z"
    generateName: mssql-
    labels:
      app: mssql
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: mssql-7fd56b764f
      statefulset.kubernetes.io/pod-name: mssql-0
    name: mssql-0
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: mssql
      uid: 82fa6c04-7701-4361-9544-86d4cbf59cd1
    resourceVersion: "1720274"
    uid: 77912096-dabe-4be1-843a-80d7a72caaa1
  spec:
    containers:
    - env:
      - name: ACCEPT_EULA
        value: "Y"
      - name: SA_PASSWORD
        valueFrom:
          secretKeyRef:
            key: SA_PASSWORD
            name: mssql-secret
      image: mcr.microsoft.com/mssql/server:2019-latest
      imagePullPolicy: IfNotPresent
      name: mssql
      ports:
      - containerPort: 1433
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/mssql
        name: mssql-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k4d2c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: mssql-0
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: mssql
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: mssql-data
      persistentVolumeClaim:
        claimName: mssql-data-mssql-0
    - name: kube-api-access-k4d2c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-22T06:52:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-22T06:52:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://424a774ecf1789b8c998f460c6a2a57032c5fb0ff3c334a93927305c1d3aa6f5
      image: mcr.microsoft.com/mssql/server:2019-latest
      imageID: mcr.microsoft.com/mssql/server@sha256:cb917712eb2c8a1a497f71a0287ef9aaccd5ce549515c2ace0ae5e734f293d3e
      lastState:
        terminated:
          containerID: containerd://62d2a2b3abeee5bd426c144c0f3523e5752c4fc79dea09630a4817427268201f
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:42Z"
      name: mssql
      ready: true
      restartCount: 9
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:08Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.10
    podIPs:
    - ip: 10.244.0.10
    qosClass: BestEffort
    startTime: "2025-07-22T06:52:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-07-22T06:40:23Z"
    generateName: my-postgres-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: my-postgres
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 17.5.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: my-postgres-postgresql-79868d6454
      helm.sh/chart: postgresql-16.7.21
      statefulset.kubernetes.io/pod-name: my-postgres-postgresql-0
    name: my-postgres-postgresql-0
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: my-postgres-postgresql
      uid: 4d775bae-788c-402b-ba94-0f3755f1e642
    resourceVersion: "1720408"
    uid: fc60e822-9844-4c8a-b659-f2c7554ffa02
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: my-postgres
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: myuser
      - name: POSTGRES_PASSWORD_FILE
        value: /opt/bitnami/postgresql/secrets/password
      - name: POSTGRES_POSTGRES_PASSWORD_FILE
        value: /opt/bitnami/postgresql/secrets/postgres-password
      - name: POSTGRES_DATABASE
        value: mydb
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:17.5.0-debian-12-r20
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "myuser" -d "dbname=mydb" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "myuser" -d "dbname=mydb" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 2Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/postgresql/conf
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/postgresql/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /opt/bitnami/postgresql/secrets/
        name: postgresql-password
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: my-postgres-postgresql-0
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: my-postgres-postgresql
    serviceAccountName: my-postgres-postgresql
    subdomain: my-postgres-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-my-postgres-postgresql-0
    - emptyDir: {}
      name: empty-dir
    - name: postgresql-password
      secret:
        defaultMode: 420
        secretName: my-postgres-postgresql
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-07-22T06:40:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-07-22T06:40:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://837020e9aed9c5f1ce7ca5027ddde3499245f7501c57009581a7a1d9e4a21ad1
      image: docker.io/bitnami/postgresql:17.5.0-debian-12-r20
      imageID: docker.io/bitnami/postgresql@sha256:42a8200d35971f931b869ef5252d996e137c6beb4b8f1b6d2181dc7d1b6f62e0
      lastState:
        terminated:
          containerID: containerd://03968227e144d7a9f06600b8e305428ca724618e52162752394543d3f64123a1
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:36Z"
      name: postgresql
      ready: true
      restartCount: 9
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:06Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.4
    podIPs:
    - ip: 10.244.0.4
    qosClass: Burstable
    startTime: "2025-07-22T06:40:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-07-24T11:11:00Z"
    creationTimestamp: "2025-08-07T08:12:30Z"
    generateName: my-webapp-69f657c96f-
    labels:
      app: my-webapp
      pod-template-hash: 69f657c96f
    name: my-webapp-69f657c96f-9ns6x
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: my-webapp-69f657c96f
      uid: 5c0f4858-af96-4f37-9644-e43abcaf3c9c
    resourceVersion: "1720269"
    uid: 9807ec83-8ff6-4418-8c9f-b1f6d025f68c
  spec:
    containers:
    - image: my-webapp:latest
      imagePullPolicy: IfNotPresent
      name: my-webapp
      ports:
      - containerPort: 8080
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kcxzn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-kcxzn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-08-07T08:12:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-08-07T08:12:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://45d9d7cb56b13bbfcb41d61dd6cad94bbefc6c7fbe27c09799c1b76658428b54
      image: docker.io/library/my-webapp:latest
      imageID: docker.io/library/import-2025-07-24@sha256:28ff51f0eb656fa85a23a408aa6843b50990ef00f6dab26ca5f7d86e6feb147f
      lastState:
        terminated:
          containerID: containerd://8ada70ea130b5a03516d262438b75f3e58c6d5f38e2e3e4b6b18cdafa19853f1
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:43Z"
      name: my-webapp
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:08Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.12
    podIPs:
    - ip: 10.244.0.12
    qosClass: BestEffort
    startTime: "2025-08-07T08:12:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-08-01T06:57:09Z"
    creationTimestamp: "2025-08-01T06:57:11Z"
    generateName: visits-service-5888d64874-
    labels:
      app: visits-service
      pod-template-hash: 5888d64874
    name: visits-service-5888d64874-h7zg7
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: visits-service-5888d64874
      uid: 47961e09-1129-47fd-a80e-316fdded0b4f
    resourceVersion: "1720266"
    uid: 4a0e1922-d8f9-47b5-9739-a1d465e7f8bc
  spec:
    containers:
    - image: my-visits-service:latest
      imagePullPolicy: Never
      name: visits-service
      ports:
      - containerPort: 8080
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jxfjh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: my-lab-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-jxfjh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-08-01T06:57:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-09-29T11:42:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-08-01T06:57:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://78873ea183057e8ba3dee493efb3d6d8454c6a378bb31b8c3f454865583b3448
      image: docker.io/library/my-visits-service:latest
      imageID: docker.io/library/import-2025-07-28@sha256:50e7c6284f2f25f80a6b168a49df5be4f81570703dbecf4afb38626e18445879
      lastState:
        terminated:
          containerID: containerd://ffb86d14f345c9e544018b4c69aedd0ad2e8df0e56d689e1a87a1c916cb796c2
          exitCode: 255
          finishedAt: "2025-09-29T11:41:17Z"
          reason: Unknown
          startedAt: "2025-08-11T10:15:42Z"
      name: visits-service
      ready: true
      restartCount: 5
      started: true
      state:
        running:
          startedAt: "2025-09-29T11:42:05Z"
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 10.244.0.2
    podIPs:
    - ip: 10.244.0.2
    qosClass: BestEffort
    startTime: "2025-08-01T06:57:14Z"
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-07-21T06:52:27Z"
    labels:
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: default
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: monitoring-kube-prometheus-alertmanager
      uid: 2340c0bf-4f10-4240-b0ec-065d843dc500
    resourceVersion: "15649"
    uid: f57d014d-35de-474a-b96c-b10237c0b042
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: 9094
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: 9094
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-07-21T03:46:40Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "230"
    uid: 6eaa5f2e-a90a-48db-b570-7f04f12e00d9
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.2
      helm.sh/chart: grafana-9.2.10
    name: monitoring-grafana
    namespace: default
    resourceVersion: "15421"
    uid: 2eec2a3b-d29b-4ba2-ba7b-0504801bf2d0
  spec:
    clusterIP: 10.96.9.210
    clusterIPs:
    - 10.96.9.210
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: 3000
    selector:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      release: monitoring
      self-monitor: "true"
    name: monitoring-kube-prometheus-alertmanager
    namespace: default
    resourceVersion: "15430"
    uid: 0c7a9468-2cf9-4356-a840-e209af0e76a8
  spec:
    clusterIP: 10.96.74.255
    clusterIPs:
    - 10.96.74.255
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: monitoring-kube-prometheus-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      release: monitoring
    name: monitoring-kube-prometheus-operator
    namespace: default
    resourceVersion: "15440"
    uid: 006b1629-682e-4dc6-8784-c4816fa96228
  spec:
    clusterIP: 10.96.228.157
    clusterIPs:
    - 10.96.228.157
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: monitoring
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      release: monitoring
      self-monitor: "true"
    name: monitoring-kube-prometheus-prometheus
    namespace: default
    resourceVersion: "15416"
    uid: 41555de4-64c2-4391-9312-537d6f484a31
  spec:
    clusterIP: 10.96.128.230
    clusterIPs:
    - 10.96.128.230
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: monitoring-kube-prometheus-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.16.0
      helm.sh/chart: kube-state-metrics-6.1.0
      release: monitoring
    name: monitoring-kube-state-metrics
    namespace: default
    resourceVersion: "15425"
    uid: af2f7497-d129-43ba-a971-25c3ac88d3a0
  spec:
    clusterIP: 10.96.28.224
    clusterIPs:
    - 10.96.28.224
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.1
      helm.sh/chart: prometheus-node-exporter-4.47.1
      jobLabel: node-exporter
      release: monitoring
    name: monitoring-prometheus-node-exporter
    namespace: default
    resourceVersion: "15436"
    uid: 8b9dded1-4355-4f1d-b513-44867d565769
  spec:
    clusterIP: 10.96.118.31
    clusterIPs:
    - 10.96.118.31
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"node-exporter"},"name":"node-exporter","namespace":"default"},"spec":{"ports":[{"port":9100,"protocol":"TCP","targetPort":9100}],"selector":{"app":"node-exporter"}}}
    creationTimestamp: "2025-07-31T06:08:57Z"
    labels:
      app: node-exporter
    name: node-exporter
    namespace: default
    resourceVersion: "572506"
    uid: 94f8ad34-6ebe-490e-b038-d1fdd5efd375
  spec:
    clusterIP: 10.96.46.43
    clusterIPs:
    - 10.96.46.43
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app: node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-07-21T06:52:29Z"
    labels:
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: default
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: monitoring-kube-prometheus-prometheus
      uid: 3dcab980-647b-41b4-b6a1-e8ddd303e8ae
    resourceVersion: "15670"
    uid: eeb2c548-c54f-4fbb-96f7-253a3a1598d9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-07-21T03:46:41Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "258"
    uid: 0e693279-d644-4b73-8e77-b2f735df8dcc
  spec:
    clusterIP: 10.96.0.10
    clusterIPs:
    - 10.96.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      jobLabel: coredns
      release: monitoring
    name: monitoring-kube-prometheus-coredns
    namespace: kube-system
    resourceVersion: "15400"
    uid: 8f80bcd5-844c-46d4-b0e1-4f3d34671848
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      jobLabel: kube-controller-manager
      release: monitoring
    name: monitoring-kube-prometheus-kube-controller-manager
    namespace: kube-system
    resourceVersion: "15402"
    uid: c14837b6-56b2-4be3-858f-103b3294e2c6
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    selector:
      component: kube-controller-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      jobLabel: kube-etcd
      release: monitoring
    name: monitoring-kube-prometheus-kube-etcd
    namespace: kube-system
    resourceVersion: "15403"
    uid: 71163251-bc87-4ee5-a3fd-3af4d894ffb9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-kube-proxy
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      jobLabel: kube-proxy
      release: monitoring
    name: monitoring-kube-prometheus-kube-proxy
    namespace: kube-system
    resourceVersion: "15404"
    uid: 0c768094-2751-4d2b-86d0-65529fcc2178
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: kube-proxy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:08Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      jobLabel: kube-scheduler
      release: monitoring
    name: monitoring-kube-prometheus-kube-scheduler
    namespace: kube-system
    resourceVersion: "15401"
    uid: 98beed79-89cb-403b-bb8c-071a153eb371
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    selector:
      component: kube-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-07-21T06:52:26Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: monitoring-kube-prometheus-kubelet
    namespace: kube-system
    resourceVersion: "15629"
    uid: f59f13fd-8c9a-4c0c-a671-57cb509996a8
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"mssql","namespace":"sre-lab"},"spec":{"ports":[{"port":1433}],"selector":{"app":"mssql"}}}
    creationTimestamp: "2025-07-22T06:51:52Z"
    name: mssql
    namespace: sre-lab
    resourceVersion: "87297"
    uid: fcdb5792-32d2-4a05-be76-60dcaaf1cc32
  spec:
    clusterIP: 10.96.157.245
    clusterIPs:
    - 10.96.157.245
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 1433
      protocol: TCP
      targetPort: 1433
    selector:
      app: mssql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: my-postgres
      meta.helm.sh/release-namespace: sre-lab
    creationTimestamp: "2025-07-22T06:40:14Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: my-postgres
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 17.5.0
      helm.sh/chart: postgresql-16.7.21
    name: my-postgres-postgresql
    namespace: sre-lab
    resourceVersion: "86366"
    uid: ea8ec117-c1f8-4496-b278-2e04b0eb17e0
  spec:
    clusterIP: 10.96.136.39
    clusterIPs:
    - 10.96.136.39
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: my-postgres
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"my-webapp","namespace":"sre-lab"},"spec":{"ports":[{"port":80,"targetPort":8080}],"selector":{"app":"my-webapp"},"type":"LoadBalancer"}}
    creationTimestamp: "2025-07-24T08:21:19Z"
    name: my-webapp
    namespace: sre-lab
    resourceVersion: "159139"
    uid: 301ee6b5-eec5-4727-a1a2-ee0e403683e5
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.96.189.205
    clusterIPs:
    - 10.96.189.205
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 30962
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: my-webapp
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"visits-service","namespace":"sre-lab"},"spec":{"ports":[{"port":8080,"targetPort":8080}],"selector":{"app":"visits-service"}}}
    creationTimestamp: "2025-07-24T07:22:50Z"
    name: visits-service
    namespace: sre-lab
    resourceVersion: "333774"
    uid: 9174d4d2-5332-49da-98ed-339b7bfd8b66
  spec:
    clusterIP: 10.96.153.12
    clusterIPs:
    - 10.96.153.12
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: web
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: visits-service
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.9.1
      helm.sh/chart: prometheus-node-exporter-4.47.1
      release: monitoring
    name: monitoring-prometheus-node-exporter
    namespace: default
    resourceVersion: "15589"
    uid: 74d257ad-f141-404b-b641-5a4b0026bba5
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.9.1
          helm.sh/chart: prometheus-node-exporter-4.47.1
          jobLabel: node-exporter
          release: monitoring
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.9.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: monitoring-prometheus-node-exporter
        serviceAccountName: monitoring-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-07-21T03:46:45Z"
    generation: 1
    labels:
      app: kindnet
      k8s-app: kindnet
      tier: node
    name: kindnet
    namespace: kube-system
    resourceVersion: "64615"
    uid: e3287825-7588-414a-b428-fc8fb2f95c71
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kindnet
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kindnet
          k8s-app: kindnet
          tier: node
      spec:
        containers:
        - env:
          - name: HOST_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_SUBNET
            value: 10.244.0.0/16
          - name: CONTROL_PLANE_ENDPOINT
            value: my-lab-control-plane:6443
          image: docker.io/kindest/kindnetd:v20240202-8f1494ea
          imagePullPolicy: IfNotPresent
          name: kindnet-cni
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            capabilities:
              add:
              - NET_RAW
              - NET_ADMIN
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cni/net.d
            name: cni-cfg
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kindnet
        serviceAccountName: kindnet
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-cfg
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-07-21T03:46:41Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "429"
    uid: 1d482b88-44ce-46e5-a54d-a7be30f3d72c
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/kube-proxy:v1.30.0
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 1
    desiredNumberScheduled: 1
    numberAvailable: 1
    numberMisscheduled: 0
    numberReady: 1
    observedGeneration: 1
    updatedNumberScheduled: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.2
      helm.sh/chart: grafana-9.2.10
    name: monitoring-grafana
    namespace: default
    resourceVersion: "1720622"
    uid: 1753b250-fab0-4165-997f-6c130ecd1fbc
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.0.2
          helm.sh/chart: grafana-9.2.10
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.3
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.3
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.0.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: monitoring-grafana
        serviceAccountName: monitoring-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: monitoring-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: monitoring-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-07-21T06:52:10Z"
      lastUpdateTime: "2025-08-01T09:52:51Z"
      message: ReplicaSet "monitoring-grafana-7cd95b6c46" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-09-29T11:44:00Z"
      lastUpdateTime: "2025-09-29T11:44:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      release: monitoring
    name: monitoring-kube-prometheus-operator
    namespace: default
    resourceVersion: "1720506"
    uid: b60ae161-b25a-4748-b808-8fe3765460bc
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: monitoring
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 75.12.0
          chart: kube-prometheus-stack-75.12.0
          heritage: Helm
          release: monitoring
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/monitoring-kube-prometheus-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.83.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: monitoring-kube-prometheus-operator
        serviceAccountName: monitoring-kube-prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: monitoring-kube-prometheus-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-07-21T06:52:10Z"
      lastUpdateTime: "2025-07-21T06:52:27Z"
      message: ReplicaSet "monitoring-kube-prometheus-operator-74f7694f79" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-09-29T11:43:10Z"
      lastUpdateTime: "2025-09-29T11:43:10Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.16.0
      helm.sh/chart: kube-state-metrics-6.1.0
      release: monitoring
    name: monitoring-kube-state-metrics
    namespace: default
    resourceVersion: "1720463"
    uid: d031a3b1-3e8d-4618-af22-319feaa7211b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.16.0
          helm.sh/chart: kube-state-metrics-6.1.0
          release: monitoring
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: monitoring-kube-state-metrics
        serviceAccountName: monitoring-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-07-21T06:52:10Z"
      lastUpdateTime: "2025-07-21T06:52:32Z"
      message: ReplicaSet "monitoring-kube-state-metrics-877c9547f" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-09-29T11:43:01Z"
      lastUpdateTime: "2025-09-29T11:43:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-07-21T03:46:41Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "1720537"
    uid: a34f4f23-b7e3-475f-b145-80c188ca0f95
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2025-07-21T03:46:55Z"
      lastUpdateTime: "2025-07-21T03:47:04Z"
      message: ReplicaSet "coredns-7db6d8ff4d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-09-29T11:43:16Z"
      lastUpdateTime: "2025-09-29T11:43:16Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"local-path-provisioner","namespace":"local-path-storage"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"local-path-provisioner"}},"template":{"metadata":{"labels":{"app":"local-path-provisioner"}},"spec":{"containers":[{"command":["local-path-provisioner","--debug","start","--helper-image","docker.io/kindest/local-path-helper:v20230510-486859a6","--config","/etc/config/config.json"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"docker.io/kindest/local-path-provisioner:v20240202-8f1494ea","imagePullPolicy":"IfNotPresent","name":"local-path-provisioner","volumeMounts":[{"mountPath":"/etc/config/","name":"config-volume"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"serviceAccountName":"local-path-provisioner-service-account","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane","operator":"Equal"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Equal"}],"volumes":[{"configMap":{"name":"local-path-config"},"name":"config-volume"}]}}}}
    creationTimestamp: "2025-07-21T03:46:45Z"
    generation: 1
    name: local-path-provisioner
    namespace: local-path-storage
    resourceVersion: "1720543"
    uid: dfb60307-ecb4-42ec-96ac-1bb269698cb2
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --helper-image
          - docker.io/kindest/local-path-helper:v20230510-486859a6
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: docker.io/kindest/local-path-provisioner:v20240202-8f1494ea
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-07-21T03:46:55Z"
      lastUpdateTime: "2025-07-21T03:47:03Z"
      message: ReplicaSet "local-path-provisioner-988d74bc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-09-29T11:43:20Z"
      lastUpdateTime: "2025-09-29T11:43:20Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "5"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"my-webapp","namespace":"sre-lab"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"my-webapp"}},"template":{"metadata":{"labels":{"app":"my-webapp"}},"spec":{"containers":[{"image":"my-webapp:latest","name":"my-webapp","ports":[{"containerPort":8080}]}]}}}}
    creationTimestamp: "2025-07-24T08:21:18Z"
    generation: 5
    name: my-webapp
    namespace: sre-lab
    resourceVersion: "1048467"
    uid: 844033f5-2d3e-44e2-b287-d07660554b3f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: my-webapp
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-24T11:11:00Z"
        creationTimestamp: null
        labels:
          app: my-webapp
      spec:
        containers:
        - image: my-webapp:latest
          imagePullPolicy: IfNotPresent
          name: my-webapp
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-07-24T08:21:20Z"
      lastUpdateTime: "2025-07-24T11:11:08Z"
      message: ReplicaSet "my-webapp-69f657c96f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2025-08-07T08:12:37Z"
      lastUpdateTime: "2025-08-07T08:12:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 5
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "9"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"visits-service","namespace":"sre-lab"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"visits-service"}},"template":{"metadata":{"labels":{"app":"visits-service"}},"spec":{"containers":[{"image":"my-visits-service:latest","imagePullPolicy":"Never","name":"visits-service","ports":[{"containerPort":8080}]}]}}}}
    creationTimestamp: "2025-07-24T07:22:50Z"
    generation: 9
    name: visits-service
    namespace: sre-lab
    resourceVersion: "686012"
    uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: visits-service
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-08-01T06:57:09Z"
        creationTimestamp: null
        labels:
          app: visits-service
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-07-24T07:33:01Z"
      lastUpdateTime: "2025-07-24T07:33:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-07-24T07:32:56Z"
      lastUpdateTime: "2025-08-01T06:57:24Z"
      message: ReplicaSet "visits-service-5888d64874" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 9
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.2
      helm.sh/chart: grafana-9.2.10
      pod-template-hash: 556b67b498
    name: monitoring-grafana-556b67b498
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: monitoring-grafana
      uid: 1753b250-fab0-4165-997f-6c130ecd1fbc
    resourceVersion: "699473"
    uid: cbecbb9f-df31-45da-afde-f867b39fe229
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/name: grafana
        pod-template-hash: 556b67b498
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.0.2
          helm.sh/chart: grafana-9.2.10
          pod-template-hash: 556b67b498
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.3
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.3
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.0.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: monitoring-grafana
        serviceAccountName: monitoring-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: monitoring-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: monitoring-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-08-01T09:44:17Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.0.2
      helm.sh/chart: grafana-9.2.10
      pod-template-hash: 7cd95b6c46
    name: monitoring-grafana-7cd95b6c46
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: monitoring-grafana
      uid: 1753b250-fab0-4165-997f-6c130ecd1fbc
    resourceVersion: "1720621"
    uid: e75cdbf5-d954-4e48-9750-040e8de25433
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/name: grafana
        pod-template-hash: 7cd95b6c46
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712
          kubectl.kubernetes.io/default-container: grafana
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.0.2
          helm.sh/chart: grafana-9.2.10
          pod-template-hash: 7cd95b6c46
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.3
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          image: quay.io/kiwigrid/k8s-sidecar:1.30.3
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: monitoring-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: monitoring-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          image: docker.io/grafana/grafana:12.0.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: monitoring-grafana
        serviceAccountName: monitoring-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: monitoring-grafana
          name: config
        - emptyDir: {}
          name: storage
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: monitoring-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      pod-template-hash: 74f7694f79
      release: monitoring
    name: monitoring-kube-prometheus-operator-74f7694f79
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: monitoring-kube-prometheus-operator
      uid: b60ae161-b25a-4748-b808-8fe3765460bc
    resourceVersion: "1720504"
    uid: e99e07bd-9f49-4ef4-8ffe-b6a69b1e8ac8
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 74f7694f79
        release: monitoring
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 75.12.0
          chart: kube-prometheus-stack-75.12.0
          heritage: Helm
          pod-template-hash: 74f7694f79
          release: monitoring
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/monitoring-kube-prometheus-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.83.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: monitoring-kube-prometheus-operator
        serviceAccountName: monitoring-kube-prometheus-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: monitoring-kube-prometheus-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
    creationTimestamp: "2025-07-21T06:52:10Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.16.0
      helm.sh/chart: kube-state-metrics-6.1.0
      pod-template-hash: 877c9547f
      release: monitoring
    name: monitoring-kube-state-metrics-877c9547f
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: monitoring-kube-state-metrics
      uid: d031a3b1-3e8d-4618-af22-319feaa7211b
    resourceVersion: "1720458"
    uid: c8cbf224-bc1c-4590-9d93-8616a3abb5f2
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 877c9547f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: monitoring
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.16.0
          helm.sh/chart: kube-state-metrics-6.1.0
          pod-template-hash: 877c9547f
          release: monitoring
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.16.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: monitoring-kube-state-metrics
        serviceAccountName: monitoring-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-07-21T03:46:55Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 7db6d8ff4d
    name: coredns-7db6d8ff4d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: a34f4f23-b7e3-475f-b145-80c188ca0f95
    resourceVersion: "1720534"
    uid: f2ee9818-0858-4ad1-bca5-98e85e80b9a2
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 7db6d8ff4d
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 7db6d8ff4d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-07-21T03:46:55Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 988d74bc
    name: local-path-provisioner-988d74bc
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: dfb60307-ecb4-42ec-96ac-1bb269698cb2
    resourceVersion: "1720541"
    uid: 1b33bd94-1612-4b8d-b627-c72daf84433c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 988d74bc
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 988d74bc
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --helper-image
          - docker.io/kindest/local-path-helper:v20230510-486859a6
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: docker.io/kindest/local-path-provisioner:v20240202-8f1494ea
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Equal
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Equal
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-07-24T11:10:56Z"
    generation: 2
    labels:
      app: my-webapp
      pod-template-hash: 5c4594959
    name: my-webapp-5c4594959
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-webapp
      uid: 844033f5-2d3e-44e2-b287-d07660554b3f
    resourceVersion: "171926"
    uid: f059053a-44fd-43d1-af73-2fa66d2f094e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: my-webapp
        pod-template-hash: 5c4594959
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-24T11:10:56Z"
        creationTimestamp: null
        labels:
          app: my-webapp
          pod-template-hash: 5c4594959
      spec:
        containers:
        - image: my-webapp:latest
          imagePullPolicy: IfNotPresent
          name: my-webapp
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-07-24T08:21:20Z"
    generation: 2
    labels:
      app: my-webapp
      pod-template-hash: 66f9d9bfbb
    name: my-webapp-66f9d9bfbb
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-webapp
      uid: 844033f5-2d3e-44e2-b287-d07660554b3f
    resourceVersion: "159740"
    uid: e605168b-268e-415c-a07c-d93de6ac0704
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: my-webapp
        pod-template-hash: 66f9d9bfbb
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: my-webapp
          pod-template-hash: 66f9d9bfbb
      spec:
        containers:
        - image: my-webapp:latest
          imagePullPolicy: Always
          name: my-webapp
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-07-24T11:11:00Z"
    generation: 2
    labels:
      app: my-webapp
      pod-template-hash: 69f657c96f
    name: my-webapp-69f657c96f
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-webapp
      uid: 844033f5-2d3e-44e2-b287-d07660554b3f
    resourceVersion: "1048466"
    uid: 5c0f4858-af96-4f37-9644-e43abcaf3c9c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: my-webapp
        pod-template-hash: 69f657c96f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-24T11:11:00Z"
        creationTimestamp: null
        labels:
          app: my-webapp
          pod-template-hash: 69f657c96f
      spec:
        containers:
        - image: my-webapp:latest
          imagePullPolicy: IfNotPresent
          name: my-webapp
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-07-24T11:01:00Z"
    generation: 2
    labels:
      app: my-webapp
      pod-template-hash: 7fbdfc6745
    name: my-webapp-7fbdfc6745
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-webapp
      uid: 844033f5-2d3e-44e2-b287-d07660554b3f
    resourceVersion: "171966"
    uid: f8a040e4-e952-4526-b93d-fa883bc86f8d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: my-webapp
        pod-template-hash: 7fbdfc6745
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-24T11:00:59Z"
        creationTimestamp: null
        labels:
          app: my-webapp
          pod-template-hash: 7fbdfc6745
      spec:
        containers:
        - image: my-webapp:latest
          imagePullPolicy: IfNotPresent
          name: my-webapp
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-07-24T08:28:36Z"
    generation: 2
    labels:
      app: my-webapp
      pod-template-hash: fbd4bb788
    name: my-webapp-fbd4bb788
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: my-webapp
      uid: 844033f5-2d3e-44e2-b287-d07660554b3f
    resourceVersion: "171189"
    uid: 8c9d93fe-f5e1-4f2e-a908-02eb383aee22
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: my-webapp
        pod-template-hash: fbd4bb788
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: my-webapp
          pod-template-hash: fbd4bb788
      spec:
        containers:
        - image: my-webapp:latest
          imagePullPolicy: IfNotPresent
          name: my-webapp
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2025-07-28T09:33:42Z"
    generation: 3
    labels:
      app: visits-service
      pod-template-hash: 5449c4c7d6
    name: visits-service-5449c4c7d6
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "319699"
    uid: fa441bbe-d7a2-4ceb-abe0-680d397c9bf6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 5449c4c7d6
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-28T09:33:42Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 5449c4c7d6
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-07-24T07:32:56Z"
    generation: 2
    labels:
      app: visits-service
      pod-template-hash: 555b5f4f98
    name: visits-service-555b5f4f98
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "319753"
    uid: 93561856-cbab-4bd3-b1ce-c4afcde92d95
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 555b5f4f98
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 555b5f4f98
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2025-07-28T11:12:58Z"
    generation: 2
    labels:
      app: visits-service
      pod-template-hash: 55c6f4c9b6
    name: visits-service-55c6f4c9b6
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "327183"
    uid: 2e0291be-1b65-44f6-8444-3e280630391e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 55c6f4c9b6
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-28T11:12:57Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 55c6f4c9b6
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2025-08-01T06:57:11Z"
    generation: 1
    labels:
      app: visits-service
      pod-template-hash: 5888d64874
    name: visits-service-5888d64874
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "685996"
    uid: 47961e09-1129-47fd-a80e-316fdded0b4f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 5888d64874
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-08-01T06:57:09Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 5888d64874
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-07-24T07:22:50Z"
    generation: 2
    labels:
      app: visits-service
      pod-template-hash: 588bbf5c
    name: visits-service-588bbf5c
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "155523"
    uid: 6c60e204-307f-47e5-9488-f57f648fca22
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 588bbf5c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 588bbf5c
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Always
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2025-07-28T11:14:05Z"
    generation: 2
    labels:
      app: visits-service
      pod-template-hash: 69c94df76d
    name: visits-service-69c94df76d
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "332071"
    uid: c60b3f21-f1c4-4804-bbc3-ebc511bedd71
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 69c94df76d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-28T11:14:04Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 69c94df76d
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-07-28T09:33:39Z"
    generation: 2
    labels:
      app: visits-service
      pod-template-hash: 746d68db75
    name: visits-service-746d68db75
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "319686"
    uid: 91bd5e4c-c150-402c-820a-2fc410f7a9ec
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 746d68db75
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-28T09:33:36Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 746d68db75
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2025-07-28T12:20:36Z"
    generation: 2
    labels:
      app: visits-service
      pod-template-hash: 798d58cd9
    name: visits-service-798d58cd9
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "686011"
    uid: c9807428-33e0-4bf1-9a45-6c6a8225d195
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 798d58cd9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-28T12:20:34Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 798d58cd9
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2025-07-28T09:33:46Z"
    generation: 3
    labels:
      app: visits-service
      pod-template-hash: 7c78c6f757
    name: visits-service-7c78c6f757
    namespace: sre-lab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: visits-service
      uid: 6dcd4000-95d8-46c7-b1e7-756097dac77f
    resourceVersion: "327021"
    uid: 69815ad1-adad-4c89-b2c0-f5e348bdcda1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: visits-service
        pod-template-hash: 7c78c6f757
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-07-28T09:33:45Z"
        creationTimestamp: null
        labels:
          app: visits-service
          pod-template-hash: 7c78c6f757
      spec:
        containers:
        - image: my-visits-service:latest
          imagePullPolicy: Never
          name: visits-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
      prometheus-operator-input-hash: "5297692769258781093"
    creationTimestamp: "2025-07-21T06:52:28Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      managed-by: prometheus-operator
      release: monitoring
    name: alertmanager-monitoring-kube-prometheus-alertmanager
    namespace: default
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: monitoring-kube-prometheus-alertmanager
      uid: 2340c0bf-4f10-4240-b0ec-065d843dc500
    resourceVersion: "1720483"
    uid: ead7667f-f75b-4b00-aacb-a9a0dbe54cc0
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: monitoring-kube-prometheus-alertmanager
        app.kubernetes.io/instance: monitoring-kube-prometheus-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: monitoring-kube-prometheus-alertmanager
          app.kubernetes.io/instance: monitoring-kube-prometheus-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.28.1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - alertmanager
                  - key: alertmanager
                    operator: In
                    values:
                    - monitoring-kube-prometheus-alertmanager
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://monitoring-kube-prometheus-alertmanager.default:9093
          - --web.route-prefix=/
          - --cluster.label=default/monitoring-kube-prometheus-alertmanager
          - --cluster.peer=alertmanager-monitoring-kube-prometheus-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.28.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-monitoring-kube-prometheus-alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
            name: cluster-tls-config
            readOnly: true
            subPath: cluster-tls-config.yaml
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: monitoring-kube-prometheus-alertmanager
        serviceAccountName: monitoring-kube-prometheus-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-monitoring-kube-prometheus-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-monitoring-kube-prometheus-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-monitoring-kube-prometheus-alertmanager-web-config
        - name: cluster-tls-config
          secret:
            defaultMode: 420
            secretName: alertmanager-monitoring-kube-prometheus-alertmanager-cluster-tls-config
        - emptyDir: {}
          name: alertmanager-monitoring-kube-prometheus-alertmanager-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-monitoring-kube-prometheus-alertmanager-57594fd46
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-monitoring-kube-prometheus-alertmanager-57594fd46
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: monitoring
      meta.helm.sh/release-namespace: default
      prometheus-operator-input-hash: "7936912943315764707"
    creationTimestamp: "2025-07-21T06:52:29Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 75.12.0
      chart: kube-prometheus-stack-75.12.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: monitoring-kube-prometheus-prometheus
      operator.prometheus.io/shard: "0"
      release: monitoring
    name: prometheus-monitoring-kube-prometheus-prometheus
    namespace: default
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: monitoring-kube-prometheus-prometheus
      uid: 3dcab980-647b-41b4-b6a1-e8ddd303e8ae
    resourceVersion: "1720594"
    uid: 4b873798-1c31-495b-b26d-68f602a59c55
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: monitoring-kube-prometheus-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: monitoring-kube-prometheus-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: monitoring-kube-prometheus-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: monitoring-kube-prometheus-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 3.5.0
          operator.prometheus.io/name: monitoring-kube-prometheus-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: monitoring-kube-prometheus-prometheus
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - prometheus
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - monitoring-kube-prometheus-prometheus
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=http://monitoring-kube-prometheus-prometheus.default:9090
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=10d
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v3.5.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-monitoring-kube-prometheus-prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
            name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
            name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.83.0
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
            name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: monitoring-kube-prometheus-prometheus
        serviceAccountName: monitoring-kube-prometheus-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-monitoring-kube-prometheus-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-monitoring-kube-prometheus-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
          name: prometheus-monitoring-kube-prometheus-prometheus-rulefiles-0
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-monitoring-kube-prometheus-prometheus-web-config
        - emptyDir: {}
          name: prometheus-monitoring-kube-prometheus-prometheus-db
    updateStrategy:
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-monitoring-kube-prometheus-prometheus-95f6cf48c
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-monitoring-kube-prometheus-prometheus-95f6cf48c
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"StatefulSet","metadata":{"annotations":{},"name":"mssql","namespace":"sre-lab"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"mssql"}},"serviceName":"mssql","template":{"metadata":{"labels":{"app":"mssql"}},"spec":{"containers":[{"env":[{"name":"ACCEPT_EULA","value":"Y"},{"name":"SA_PASSWORD","valueFrom":{"secretKeyRef":{"key":"SA_PASSWORD","name":"mssql-secret"}}}],"image":"mcr.microsoft.com/mssql/server:2019-latest","name":"mssql","ports":[{"containerPort":1433}],"volumeMounts":[{"mountPath":"/var/opt/mssql","name":"mssql-data"}]}]}},"volumeClaimTemplates":[{"metadata":{"name":"mssql-data"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}}}}]}}
    creationTimestamp: "2025-07-22T06:51:52Z"
    generation: 1
    name: mssql
    namespace: sre-lab
    resourceVersion: "87584"
    uid: 82fa6c04-7701-4361-9544-86d4cbf59cd1
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: mssql
    serviceName: mssql
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mssql
      spec:
        containers:
        - env:
          - name: ACCEPT_EULA
            value: "Y"
          - name: SA_PASSWORD
            valueFrom:
              secretKeyRef:
                key: SA_PASSWORD
                name: mssql-secret
          image: mcr.microsoft.com/mssql/server:2019-latest
          imagePullPolicy: IfNotPresent
          name: mssql
          ports:
          - containerPort: 1433
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/mssql
            name: mssql-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: mssql-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: mssql-7fd56b764f
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: mssql-7fd56b764f
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: my-postgres
      meta.helm.sh/release-namespace: sre-lab
    creationTimestamp: "2025-07-22T06:40:19Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: my-postgres
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 17.5.0
      helm.sh/chart: postgresql-16.7.21
    name: my-postgres-postgresql
    namespace: sre-lab
    resourceVersion: "1720414"
    uid: 4d775bae-788c-402b-ba94-0f3755f1e642
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: my-postgres
        app.kubernetes.io/name: postgresql
    serviceName: my-postgres-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: my-postgres
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 17.5.0
          helm.sh/chart: postgresql-16.7.21
        name: my-postgres-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: my-postgres
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: myuser
          - name: POSTGRES_PASSWORD_FILE
            value: /opt/bitnami/postgresql/secrets/password
          - name: POSTGRES_POSTGRES_PASSWORD_FILE
            value: /opt/bitnami/postgresql/secrets/postgres-password
          - name: POSTGRES_DATABASE
            value: mydb
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:17.5.0-debian-12-r20
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "myuser" -d "dbname=mydb" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "myuser" -d "dbname=mydb" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/postgresql/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/postgresql/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /opt/bitnami/postgresql/secrets/
            name: postgresql-password
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: my-postgres-postgresql
        serviceAccountName: my-postgres-postgresql
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            defaultMode: 420
            secretName: my-postgres-postgresql
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: my-postgres-postgresql-79868d6454
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: my-postgres-postgresql-79868d6454
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
